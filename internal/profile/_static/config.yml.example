# Directory containing GeoIP databases for stacks managed by elastic-agent.
# stack.geoip_dir: "/path/to/geoip_dir/"
## Elastic Cloud
# Host URL
# stack.elastic_cloud.host: https://cloud.elastic.co

## Serverless stack provider
# Project type
# stack.serverless.type: observability
# Region where the Serverless project is going to be created
# stack.serverless.region: aws-us-east-1

## Enable apm-server
# Flag to enable apm-server in elastic-package stack profile config
# stack.apm_enabled: true

## Logs DB
# Flag to enable the logs index mode in logs data stream.
# stack.logsdb_enabled: true

## Enable logstash for testing
# Flag to enable logstash in elastic-package stack profile config
# stack.logstash_enabled: true

## Specify agent ports to publish
## port definition schema https://docs.docker.com/compose/compose-file/compose-file-v2/#ports
# stack.agent.ports:
# - 127.0.0.1:1514:1514/udp

## Set license subscription
# stack.elastic_subscription: "basic"

## LLM Agent Configuration
## Configure LLM providers for documentation generation and other AI-assisted features

## Amazon Bedrock
# Amazon Bedrock API key for LLM services
# llm.bedrock.api_key: "your-bedrock-api-key"
# AWS region for Bedrock services (optional, defaults to us-east-1)
# llm.bedrock.region: "us-east-1"
# Bedrock model ID (optional, defaults to anthropic.claude-3-5-sonnet-20241022-v2:0)
# llm.bedrock.model: "anthropic.claude-3-5-sonnet-20241022-v2:0"

## Google AI Studio (Gemini)
# Google AI Studio API key for Gemini models
# llm.gemini.api_key: "your-gemini-api-key"
# Gemini model ID (optional, defaults to gemini-2.5-pro)
# llm.gemini.model: "gemini-2.5-pro"

## Local LLM Provider
# Local LLM endpoint for OpenAI-compatible servers (Ollama, LocalAI, etc.)
# llm.local.endpoint: "http://localhost:11434"
# Local LLM model name (optional, defaults to llama2)
# llm.local.model: "llama2"
# Local LLM API key (optional, for servers requiring authentication)
# llm.local.api_key: "your-local-api-key"
